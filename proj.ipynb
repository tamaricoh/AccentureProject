{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Conda Environment with Python 3.12 and Preparing for Model Development  \n",
    "\n",
    "##### Step 1 - Create and Activate a Conda Environment  \n",
    "1. Create a new Conda environment with Python 3.12 using the following command:  \n",
    "   ```bash  \n",
    "   conda create -n llm_env python=3.12  \n",
    "   ```  \n",
    "2. Activate the environment:  \n",
    "   ```bash  \n",
    "   conda activate llm_env  \n",
    "   ```  \n",
    "3. Connect the environment to the Python kernel in Jupyter or VSCode to ensure it is ready for development.\n",
    "\n",
    "##### Step 2 - Install Required Libraries  \n",
    "1. Use Conda to install all necessary libraries for the project. Include any additional channels if required.  \n",
    "\n",
    "##### Step 3 - Verify the Setup  \n",
    "1. Open a notebook or script and try importing the installed libraries.  \n",
    "2. Ensure there are no import errors to confirm that the environment is properly configured and ready for building and training the machine learning model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datetime\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output file naming\n",
    "Timestamp-based naming convention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_model_path = f\"models/model_{timestamp}.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Device for Model Training  \n",
    "The variable `device` is set to use the GPU (`cuda`) if available, otherwise defaults to the CPU. This ensures the model runs on the most suitable hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 4\n",
    "learning_rate = 5e-5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom BERT Model for Sequence Classification\n",
    "\n",
    "This is a custom implementation of a BERT-based model for sequence classification. The model uses the `BertForSequenceClassification` class from Hugging Face's `transformers` library. Here's how the model works:\n",
    "\n",
    "- **Initialization**: The model is initialized with the `BertForSequenceClassification` pre-trained model (`bert-base-uncased`) and an optional number of output labels (`num_labels`).\n",
    "  \n",
    "- **Forward Pass**: During the forward pass, the input IDs and attention mask are passed through the BERT model. If labels are provided, the model computes the loss using `CrossEntropyLoss`. \n",
    "\n",
    "- **Outputs**: The model returns a dictionary containing:\n",
    "  - `loss`: Computed if labels are provided.\n",
    "  - `logits`: The raw predictions (before softmax) for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        # Use BertForSequenceClassification directly\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = outputs.loss\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation for Text Classification\n",
    "\n",
    "This function prepares a dataset for text classification by:\n",
    "1. **Tokenizing** the 'Example Description' column with padding/truncation to a fixed length (512 tokens).\n",
    "2. **Mapping labels** from 'Artifact Id' to indices based on `filtered_labels_balance_list`.\n",
    "3. **Creating a `TensorDataset`** with input IDs, attention mask, and labels for model training.\n",
    "\n",
    "The output is a PyTorch `TensorDataset` ready for use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, tokenizer):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "def create_dataset(df, tokenizer, label_mapping):\n",
    "    texts = df['Example Description'].tolist() \n",
    "    encodings = tokenize_data(texts, tokenizer)\n",
    "\n",
    "    labels = torch.tensor([label_mapping.get(x, -1) for x in df['Artifact Id']])  # Default to -1 for unknown labels\n",
    "    \n",
    "    input_ids = encodings['input_ids']\n",
    "    attention_mask = encodings['attention_mask']\n",
    "    \n",
    "    dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Trains the model for a specified number of epochs using the provided data loader, optimizer, scheduler, and device. \n",
    "\n",
    "**Input Parameters**:\n",
    "- `model`: The neural network model to be trained.\n",
    "- `train_loader`: A PyTorch DataLoader containing the training data.\n",
    "- `optimizer`: The optimization algorithm used to update the model's weights.\n",
    "- `device`: The hardware device (CPU or GPU) where the model and data will be processed.\n",
    "- `num_epochs`: The number of epochs to train the model.\n",
    "\n",
    "The function performs the following:\n",
    "\n",
    "1. **Training**: For each batch, the model computes the loss, updates parameters, and tracks accuracy and F1 score.\n",
    "2. **Metrics**: After each epoch, the loss, training accuracy, and F1 score are printed.\n",
    "\n",
    "This function does not include validation; it is a former version that focuses solely on training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            logits = outputs['logits']\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Store predictions and labels for F1 score calculation\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "            # Accuracy calculation\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate metrics for the epoch\n",
    "        accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Validation\n",
    "\n",
    "The `train_with_validation` function trains the model for several epochs while evaluating it on a validation set after each epoch.\n",
    "\n",
    "1. **Training**: For each batch, the model computes the loss, updates parameters, and tracks accuracy and F1 score.\n",
    "2. **Validation**: After each epoch, the model's performance is evaluated on the validation set, calculating validation accuracy and F1 score.\n",
    "3. **Metrics**: After each epoch, the training loss, accuracy, and F1 score are displayed, along with the validation accuracy and validation F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_validation(model, train_loader, val_loader, optimizer, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            logits = outputs['logits']\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate training metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "        val_all_labels = []\n",
    "        val_all_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}/{num_epochs}\"):\n",
    "                input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs['logits']\n",
    "\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                val_all_preds.extend(preds.cpu().tolist())\n",
    "                val_all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "                correct = (preds == labels).sum().item()\n",
    "                val_correct += correct\n",
    "                val_samples += labels.size(0)\n",
    "\n",
    "        val_accuracy = accuracy_score(val_all_labels, val_all_preds) * 100\n",
    "        val_f1 = f1_score(val_all_labels, val_all_preds, average=\"weighted\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.6f}, Accuracy: {accuracy:.6f}%, F1 Score: {f1:.6f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.6f}%, Validation F1 Score: {val_f1:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "After training, the model is evaluated on the test set to assess its performance on unseen data. During this phase, the model is set to evaluation mode using `model.eval()`, and predictions are made on the test set.\n",
    "\n",
    "- **Prediction**: For each batch in the test set, the model generates predictions (logits), which are converted to class labels using `torch.argmax()`.\n",
    "  \n",
    "- **F1 Score**: The F1 score, calculated using the `f1_score` function from `sklearn.metrics`, is reported to measure the weighted average performance across all classes.\n",
    "  \n",
    "- **Accuracy**: The accuracy of the model is calculated by comparing the predicted labels to the true labels and computing the percentage of correct predictions.\n",
    "\n",
    "The evaluation results help in understanding the model's effectiveness and generalization to the test set.\n",
    "\n",
    "*Note: This evaluation process does not update the model parameters, ensuring that it is a true test of performance.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score (weighted) for multiclass classification\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions) * 100\n",
    "\n",
    "    print(f\"F1 Score (Weighted): {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    return predictions, true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Handling\n",
    "\n",
    "- **`df`**: The full dataset loaded from the CSV (`dataset.csv`).\n",
    "- **`labels`**: The list of labels (i.e., 'Artifact Id') extracted from the dataset.\n",
    "- **`label_counts`**: The count of occurrences of each unique label in the dataset.\n",
    "- **`filtered_labels_at_least_5`**: The labels that appear at least 5 times in the dataset.\n",
    "- **`filtered_labels_at_least_5_list`**: A list of labels that appear at least 5 times.\n",
    "- **`filtered_df`**: The filtered DataFrame containing only the rows with labels that appear at least 5 times.\n",
    "\n",
    "By running this script, you get a new DataFrame, `filtered_df`, containing only the rows with labels that appear 5 or more times in the original dataset.\n",
    "\n",
    "**Note**: This is a very simple preprocessing step where we filter out labels with fewer than 5 occurrences. There is no further data cleaning, such as handling missing values or data normalization, applied at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact Id\n",
      "d3f:Command                      250\n",
      "d3f:Database                      16\n",
      "d3f:Software                      16\n",
      "d3f:HardwareDriver                14\n",
      "d3f:DisplayServer                 11\n",
      "d3f:OperatingSystem                8\n",
      "d3f:FileSystem                     7\n",
      "d3f:BootLoader                     6\n",
      "d3f:InterprocessCommunication      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('csv/dataset.csv')\n",
    "\n",
    "labels = df['Artifact Id']\n",
    "\n",
    "label_counts = labels.value_counts()\n",
    "\n",
    "filtered_labels_at_least_5 = label_counts[label_counts >= 5]\n",
    "\n",
    "filtered_labels_at_least_5_list = filtered_labels_at_least_5.index.tolist()\n",
    "\n",
    "filtered_df = df[df['Artifact Id'].isin(filtered_labels_at_least_5_list)]\n",
    "\n",
    "print(filtered_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the Dataset\n",
    "\n",
    "To address class imbalance in the dataset, we performed the following steps:\n",
    "\n",
    "1. **Random Sampling of 'Command' Class**: We noticed that the **'d3f:Command'** class was underrepresented in the dataset. Therefore, we **randomly sampled 16 instances** from this class to ensure it is adequately represented.\n",
    "\n",
    "2. **Combining the Data**: After sampling the 'Command' class, we combined it with the filtered dataset to create a **more balanced dataset**.\n",
    "\n",
    "This process ensures that the model is not biased towards the larger classes and helps it generalize better across all classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact Id\n",
      "d3f:Database                     16\n",
      "d3f:Software                     16\n",
      "d3f:Command                      16\n",
      "d3f:HardwareDriver               14\n",
      "d3f:DisplayServer                11\n",
      "d3f:OperatingSystem               8\n",
      "d3f:FileSystem                    7\n",
      "d3f:BootLoader                    6\n",
      "d3f:InterprocessCommunication     5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "filtered_labels_balance = label_counts[(label_counts >= 5) & (label_counts <= 200)]\n",
    "\n",
    "filtered_labels_balance_list = filtered_labels_balance.index.tolist()\n",
    "\n",
    "filtered_balance_df = df[df['Artifact Id'].isin(filtered_labels_balance_list)]\n",
    "\n",
    "command_df = df[df['Artifact Id'] == 'd3f:Command']\n",
    "sample_size = min(len(command_df), 16)\n",
    "sampled_command_df = command_df.sample(n=sample_size, random_state=42)\n",
    "combined_df = pd.concat([filtered_balance_df, sampled_command_df])\n",
    "\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(combined_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {label: idx for idx, label in enumerate(filtered_labels_at_least_5_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Split for Training and Testing  \n",
    "The dataset `combined_df` is split into **training/validation** and **test** sets with:  \n",
    "- **20% test size**  \n",
    "- **Stratification by `Artifact Id`**  \n",
    "\n",
    "After the split, the distribution of each subset is printed to verify stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, test_df = train_test_split(combined_df,\n",
    "                                         test_size=0.2,\n",
    "                                         stratify=combined_df['Artifact Id'],\n",
    "                                         random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df,\n",
    "                                         test_size=0.2,\n",
    "                                         stratify=train_val_df['Artifact Id'],\n",
    "                                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set label distribution:\n",
      "Artifact Id\n",
      "d3f:Database                     11\n",
      "d3f:Software                     10\n",
      "d3f:Command                      10\n",
      "d3f:HardwareDriver                9\n",
      "d3f:DisplayServer                 7\n",
      "d3f:OperatingSystem               5\n",
      "d3f:BootLoader                    4\n",
      "d3f:FileSystem                    4\n",
      "d3f:InterprocessCommunication     3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set label distribution:\")\n",
    "print(train_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set label distribution:\n",
      "Artifact Id\n",
      "d3f:Software                     3\n",
      "d3f:Command                      3\n",
      "d3f:Database                     2\n",
      "d3f:DisplayServer                2\n",
      "d3f:HardwareDriver               2\n",
      "d3f:OperatingSystem              1\n",
      "d3f:FileSystem                   1\n",
      "d3f:InterprocessCommunication    1\n",
      "d3f:BootLoader                   1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set label distribution:\")\n",
    "print(val_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set label distribution:\n",
      "Artifact Id\n",
      "d3f:Command                      3\n",
      "d3f:Database                     3\n",
      "d3f:Software                     3\n",
      "d3f:HardwareDriver               3\n",
      "d3f:OperatingSystem              2\n",
      "d3f:DisplayServer                2\n",
      "d3f:FileSystem                   2\n",
      "d3f:InterprocessCommunication    1\n",
      "d3f:BootLoader                   1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set label distribution:\")\n",
    "print(test_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Optimizer Setup\n",
    "\n",
    "This code initializes the model, optimizer, and tokenizer for training:\n",
    "1. **Model**: `CustomBertModel` is initialized with the number of labels set to the length of `filtered_labels_at_least_5_list`.\n",
    "2. **Optimizer**: AdamW is used for optimization with the model's parameters and a specified learning rate.\n",
    "3. **Tokenizer**: The BERT tokenizer (`bert-base-uncased`) is loaded for tokenizing the input text.\n",
    "4. The model is then moved to the specified **device** (GPU or CPU).\n",
    "\n",
    "The setup is ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomBertModel(\n",
       "  (bert): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomBertModel(num_labels=len(filtered_labels_at_least_5_list))\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader Creation\n",
    "\n",
    "This code prepares the datasets and DataLoaders for training, validation, and testing:\n",
    "\n",
    "1. **Datasets**: \n",
    "   - The `create_dataset` function is called to process the training (`train_df`), validation (`val_df`), and testing (`test_df`) data, using the BERT tokenizer and a label list (`filtered_labels_at_least_5_list`).\n",
    "\n",
    "2. **DataLoaders**:\n",
    "   - `train_loader`: Uses a `RandomSampler` to shuffle data for training.\n",
    "   - `val_loader` and `test_loader`: Use a `SequentialSampler` to load validation and test data sequentially.\n",
    "   - The batch size is specified by `batch_size`.\n",
    "\n",
    "These DataLoaders are ready to be used for model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_df, tokenizer, label_mapping)\n",
    "val_dataset = create_dataset(val_df, tokenizer, label_mapping)\n",
    "test_dataset = create_dataset(test_df, tokenizer, label_mapping)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee03d0ec5204d4e835e30c5f2967a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1/4:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cba8183e71434fb68d03e5ecb9871f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 1/4:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.270321, Accuracy: 14.285714%, F1 Score: 0.060647\n",
      "Validation Accuracy: 12.500000%, Validation F1 Score: 0.027778\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d76676db349410cb833c3259c943732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2/4:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf7596f3518403facaa150dfc69b005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 2/4:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2.072563, Accuracy: 23.809524%, F1 Score: 0.161118\n",
      "Validation Accuracy: 31.250000%, Validation F1 Score: 0.196429\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6fa9201f1e4f87b3ce262339f8a9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3/4:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e747739c92b64c49b5cb7e4537d5ecd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 3/4:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.907913, Accuracy: 38.095238%, F1 Score: 0.299060\n",
      "Validation Accuracy: 56.250000%, Validation F1 Score: 0.435714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ddd4e1cf0e74cbfa62dfca4ed0f3471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4/4:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9fa5d4b6164d039a0d56ea3fa1cdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 4/4:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1.674347, Accuracy: 68.253968%, F1 Score: 0.606885\n",
      "Validation Accuracy: 56.250000%, Validation F1 Score: 0.494792\n"
     ]
    }
   ],
   "source": [
    "train_with_validation(model, train_loader, val_loader, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model_2024-12-16_14-53-35.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), output_model_path)\n",
    "print(f\"Model saved to {output_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_model = CustomBertModel(num_labels=len(filtered_labels_at_least_5_list))\n",
    "\n",
    "state_dict = torch.load(output_model_path, weights_only=True)\n",
    "_model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model\n",
    "\n",
    "We evaluate both the trained model and the loaded model to ensure that they produce the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89be7eef8b964c229e3a72d8ea89e276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.4110\n",
      "Accuracy: 50.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab241797063844608a9d7a033fe3a370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.4110\n",
      "Accuracy: 50.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0, 3, 1, 0, 4, 2, 0, 3, 1, 0, 1, 3, 3, 1, 0, 1, 1, 1, 3, 1],\n",
       " [0, 5, 1, 0, 4, 2, 0, 3, 8, 7, 3, 4, 2, 1, 6, 5, 6, 2, 3, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_loader, device)\n",
    "test(_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Weights Comparison\n",
    "\n",
    "We compare the weights of the trained model and the loaded model to verify if they are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All model weights match!\n"
     ]
    }
   ],
   "source": [
    "all_weights_match = True  # Flag to track if all weights match\n",
    "\n",
    "for (name1, param1), (name2, param2) in zip(model.named_parameters(), _model.named_parameters()):\n",
    "    # Check if parameter names match (optional, but useful for debugging)\n",
    "    if name1 != name2:\n",
    "        print(f\"Parameter mismatch: {name1} vs {name2}\")\n",
    "        all_weights_match = False\n",
    "        continue\n",
    "    \n",
    "    # Compare parameter values\n",
    "    if not torch.allclose(param1, param2, atol=1e-6):\n",
    "        print(f\"Mismatch found in parameter: {name1}\")\n",
    "        all_weights_match = False\n",
    "\n",
    "if all_weights_match:\n",
    "    print(\"All model weights match!\")\n",
    "else:\n",
    "    print(\"Some weights do not match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to test_data.csv\n"
     ]
    }
   ],
   "source": [
    "test_df.to_csv('csv/test_data.csv', index=False)\n",
    "print(\"DataFrame saved to test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell compares the **trained model's predictions** with the **baseline model's predictions** to evaluate performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 18.7500\n",
      "Baseline F1 Score: 0.2083\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1134ed7bb0064e978b7927b5cd74d769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.5568\n",
      "Accuracy: 62.50%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c3fb54657c4cdcaa05696a1ce8757e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.5568\n",
      "Accuracy: 62.50%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1, 2, 1, 4, 2, 2, 0, 3, 2, 3, 2, 1, 2, 1, 4, 2],\n",
       " [5, 2, 1, 4, 1, 2, 6, 3, 2, 3, 8, 5, 2, 1, 4, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"csv/baseline.csv\"\n",
    "baseline_df = pd.read_csv(csv_path)\n",
    "\n",
    "def test_baseline (df):\n",
    "    true_labels = df['Artifact Id'].str.replace('d3f:', '', regex=False).tolist()\n",
    "    predicted_labels = df['Prediction'].tolist()\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels) * 100\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    # Print results\n",
    "    print(f\"Baseline Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Baseline F1 Score: {f1:.4f}\")\n",
    "\n",
    "baseline_dataset = create_dataset(baseline_df, tokenizer, label_mapping)\n",
    "baseline_loader = DataLoader(baseline_dataset, sampler=SequentialSampler(baseline_dataset), batch_size=batch_size)\n",
    "\n",
    "test_baseline(baseline_df)\n",
    "test(model, baseline_loader, device)\n",
    "test(_model, baseline_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
