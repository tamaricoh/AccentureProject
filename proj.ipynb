{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Conda Environment with Python 3.12 and Installing Required Libraries\n",
    "##### Step 1 - Created a Conda Environment with Python 3.12 and Connected to its Kernel\n",
    "I created a new Conda environment with Python 3.12 and activated it. After that, I connected to the Python kernel in Jupyter/VSCode using this environment.\n",
    "\n",
    "Commands to create and activate the environment:\n",
    "```bash\n",
    "conda create -n llm_env python=3.12\n",
    "conda activate llm_env\n",
    "```\n",
    "\n",
    "##### Step 2 - Imported Required Libraries\n",
    "Once the environment was set up and activated, I imported the following libraries for my project:\n",
    "- `numpy` as `np`: For numerical operations and working with arrays.\n",
    "- `pandas` as `pd`: For data manipulation and analysis.\n",
    "- `torch`: PyTorch, for deep learning model building and training.\n",
    "- `tqdm.notebook`: For displaying progress bars in Jupyter notebooks.\n",
    "- `sklearn.preprocessing.LabelEncoder`: For encoding categorical labels into numerical format.\n",
    "- `sklearn.model_selection.train_test_split`: For splitting datasets into training and testing sets.\n",
    "- `transformers.BertTokenizer`: For tokenizing text data for BERT models.\n",
    "- `transformers.BertForSequenceClassification`: For using BERT for sequence classification tasks.\n",
    "- `torch.utils.data.TensorDataset`: For creating datasets from tensors.\n",
    "- `torch.utils.data.DataLoader`: For batching datasets and loading them efficiently during training.\n",
    "- `torch.utils.data.RandomSampler` and `torch.utils.data.SequentialSampler`: For random and sequential sampling of datasets.\n",
    "- `transformers.AdamW` and `transformers.get_linear_schedule_with_warmup`: For the AdamW optimizer and learning rate scheduler used with transformers.\n",
    "- `sklearn.metrics.f1_score`: For evaluating model performance using the F1 score.\n",
    "- `random`: For random number generation, typically used in data shuffling.\n",
    "\n",
    "Commands to install the libraries:\n",
    "```bash\n",
    "conda install numpy pandas scikit-learn tqdm pytorch\n",
    "conda install -c huggingface transformers\n",
    "```\n",
    "\n",
    "Once the libraries were installed, I successfully imported them into the notebook and was ready to proceed with building and training the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import datetime\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 4\n",
    "learning_rate = 5e-5\n",
    "num_folds  = 5 \n",
    "batch_size = 16\n",
    "# output_model_path = \"bert_model_final.pth\"\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_model_path = f\"model_{timestamp}.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Handling\n",
    "\n",
    "- **`df`**: The full dataset loaded from the CSV (`dataset.csv`).\n",
    "- **`labels`**: The list of labels (i.e., 'Artifact Id') extracted from the dataset.\n",
    "- **`label_counts`**: The count of occurrences of each unique label in the dataset.\n",
    "- **`filtered_labels`**: The labels that appear at least 5 times in the dataset.\n",
    "- **`filtered_labels_list`**: A list of labels that appear at least 5 times.\n",
    "- **`filtered_df`**: The filtered DataFrame containing only the rows with labels that appear at least 5 times.\n",
    "\n",
    "By running this script, you get a new DataFrame, `filtered_df`, containing only the rows with labels that appear 5 or more times in the original dataset.\n",
    "\n",
    "**Note**: This is a very simple preprocessing step where we filter out labels with fewer than 5 occurrences. There is no further data cleaning, such as handling missing values or data normalization, applied at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# labels = df['Artifact Id']\n",
    "\n",
    "# label_counts = labels.value_counts()\n",
    "\n",
    "# filtered_labels = label_counts[label_counts >= 5]\n",
    "\n",
    "# filtered_labels_list = filtered_labels.index.tolist()\n",
    "\n",
    "# filtered_df = df[df['Artifact Id'].isin(filtered_labels_list)]\n",
    "\n",
    "# print(filtered_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for Overfitting on a Small Portion of the Data\n",
    "\n",
    "In this part of the code, we filter the dataset to only include labels that have exactly **16 samples**. By training the model on this small subset, we aim to observe if it achieves **overfitting**, where it memorizes the data rather than generalizing well. This helps us test the model's ability to avoid overfitting on limited data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# labels = df['Artifact Id']\n",
    "\n",
    "# label_counts = labels.value_counts()\n",
    "\n",
    "# filtered_labels = label_counts[label_counts == 16]\n",
    "\n",
    "# filtered_labels_list = filtered_labels.index.tolist()\n",
    "\n",
    "# filtered_df = df[df['Artifact Id'].isin(filtered_labels_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the Dataset\n",
    "\n",
    "To address class imbalance in the dataset, we performed the following steps:\n",
    "\n",
    "1. **Random Sampling of 'Command' Class**: We noticed that the **'d3f:Command'** class was underrepresented in the dataset. Therefore, we **randomly sampled 16 instances** from this class to ensure it is adequately represented.\n",
    "\n",
    "2. **Combining the Data**: After sampling the 'Command' class, we combined it with the filtered dataset to create a **more balanced dataset**.\n",
    "\n",
    "This process ensures that the model is not biased towards the larger classes and helps it generalize better across all classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact Id\n",
      "d3f:Database                     16\n",
      "d3f:Software                     16\n",
      "d3f:Command                      16\n",
      "d3f:HardwareDriver               14\n",
      "d3f:DisplayServer                11\n",
      "d3f:OperatingSystem               8\n",
      "d3f:FileSystem                    7\n",
      "d3f:BootLoader                    6\n",
      "d3f:InterprocessCommunication     5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "labels = df['Artifact Id']\n",
    "\n",
    "label_counts = labels.value_counts()\n",
    "\n",
    "filtered_labels = label_counts[(label_counts >= 5) & (label_counts <= 200)] # Remove \"Command\" label\n",
    "\n",
    "filtered_labels_list = filtered_labels.index.tolist()\n",
    "\n",
    "filtered_df = df[df['Artifact Id'].isin(filtered_labels_list)]\n",
    "\n",
    "command_df = df[df['Artifact Id'] == 'd3f:Command']\n",
    "sample_size = min(len(command_df), 16)\n",
    "sampled_command_df = command_df.sample(n=sample_size, random_state=42)\n",
    "combined_df = pd.concat([filtered_df, sampled_command_df])\n",
    "\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(combined_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, test_df = train_test_split(filtered_df,\n",
    "                                         test_size=0.2,\n",
    "                                         stratify=filtered_df['Artifact Id'],\n",
    "                                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training set label distribution:\")\n",
    "# print(train_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nValidation set label distribution:\")\n",
    "# print(val_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nTest set label distribution:\")\n",
    "# print(test_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, tokenizer):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, scheduler, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Store predictions and labels for F1 score calculation\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "            # Accuracy calculation\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate metrics for the epoch\n",
    "        accuracy = total_correct / total_samples * 100\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Validation\n",
    "\n",
    "The `train_with_validation` function trains the model for multiple epochs and evaluates it on a validation set after each epoch. \n",
    "\n",
    "1. **Training**: For each batch, the model computes the loss, updates parameters, and tracks accuracy and F1 score.\n",
    "2. **Validation**: After each epoch, the model's performance is evaluated on the validation set, and validation accuracy and F1 score are calculated.\n",
    "\n",
    "Metrics (training and validation accuracy, F1 score) are printed after each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_validation(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate training metrics\n",
    "        accuracy = total_correct / total_samples * 100\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "        val_all_labels = []\n",
    "        val_all_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}/{num_epochs}\"):\n",
    "                input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                val_all_preds.extend(preds.cpu().tolist())\n",
    "                val_all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "                correct = (preds == labels).sum().item()\n",
    "                val_correct += correct\n",
    "                val_samples += labels.size(0)\n",
    "\n",
    "        val_accuracy = val_correct / val_samples * 100\n",
    "        val_f1 = f1_score(val_all_labels, val_all_preds, average=\"weighted\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.6f}, Accuracy: {accuracy:.6f}%, F1 Score: {f1:.6f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.6f}%, Validation F1 Score: {val_f1:.6f}\")\n",
    "\n",
    "    return val_accuracy, val_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "After training, the model is evaluated on the test set to assess its performance on unseen data. During this phase, the model is set to evaluation mode using `model.eval()`, and predictions are made on the test set.\n",
    "\n",
    "- **Prediction**: For each batch in the test set, the model generates predictions (logits), which are converted to class labels using `torch.argmax()`.\n",
    "  \n",
    "- **F1 Score**: The F1 score, calculated using the `f1_score` function from `sklearn.metrics`, is reported to measure the weighted average performance across all classes.\n",
    "  \n",
    "- **Accuracy**: The accuracy of the model is calculated by comparing the predicted labels to the true labels and computing the percentage of correct predictions.\n",
    "\n",
    "The evaluation results help in understanding the model's effectiveness and generalization to the test set.\n",
    "\n",
    "*Note: This evaluation process does not update the model parameters, ensuring that it is a true test of performance.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score (weighted) for multiclass classification\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(true_labels)) / len(true_labels) * 100\n",
    "\n",
    "    print(f\"F1 Score (Weighted): {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### K-Fold Cross Validation with BERT\n",
    "\n",
    "This code performs **K-Fold Cross Validation** using **StratifiedKFold** for evaluating a BERT-based model on a classification task. For each fold, it:\n",
    "\n",
    "1. Splits the dataset into training and validation sets.\n",
    "2. Tokenizes the input text (e.g., \"Example Description\").\n",
    "3. Prepares and loads data into `DataLoader` for training and validation.\n",
    "4. Initializes the BERT model and optimizer, and trains the model for a specified number of epochs.\n",
    "5. Evaluates the model on the validation set using **F1 score** (weighted) and **accuracy**.\n",
    "6. Computes average metrics across all folds.\n",
    "\n",
    "Finally, the trained model is evaluated on a test set, and the final performance metrics are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenize_data(test_df['Example Description'].tolist(), tokenizer)\n",
    "label_mapping = {label: idx for idx, label in enumerate(filtered_labels_list)}\n",
    "test_labels = torch.tensor([label_mapping.get(x, -1) for x in test_df['Artifact Id']])\n",
    "test_dataset = TensorDataset(\n",
    "    test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(\n",
    "    test_dataset), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f464e34efb3b418abc5d30371f5b4b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1/4:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc28d3f4325417da48dd3844bbaf95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 1/4:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.142305, Accuracy: 17.307692%, F1 Score: 0.122151\n",
      "Validation Accuracy: 21.428571%, Validation F1 Score: 0.133333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196c1f0489664cb2a8c632b7c06f8dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2/4:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 46\u001b[0m\n\u001b[0;32m     41\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m get_linear_schedule_with_warmup(\n\u001b[0;32m     42\u001b[0m     optimizer, num_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, num_training_steps\u001b[38;5;241m=\u001b[39mtotal_steps\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Train the model with validation\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m accuracy, f1 \u001b[38;5;241m=\u001b[39m train_with_validation(\n\u001b[0;32m     47\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     48\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m     49\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m     50\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     51\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[0;32m     52\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     53\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs\n\u001b[0;32m     54\u001b[0m )\n\u001b[0;32m     56\u001b[0m fold_results\u001b[38;5;241m.\u001b[39mappend((accuracy, f1))\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f1 \u001b[38;5;241m>\u001b[39m best_val_f1:\n",
      "Cell \u001b[1;32mIn[117], line 28\u001b[0m, in \u001b[0;36mtrain_with_validation\u001b[1;34m(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[0;32m     25\u001b[0m total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m correct\n\u001b[0;32m     26\u001b[0m total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # K-Fold Cross Validation on 80% train/val data\n",
    "# best_model = None\n",
    "# best_val_f1 = 0.0\n",
    "# fold_results = []  # Store F1 score and accuracy for each fold\n",
    "# skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "# for fold, (train_index, val_index) in enumerate(skf.split(train_val_df, train_val_df['Artifact Id'])):\n",
    "#     print(f\"\\n--- Fold {fold+1}/{num_folds} ---\")\n",
    "\n",
    "#     # Split the train/val set into training and validation data for this fold\n",
    "#     train_df = train_val_df.iloc[train_index]\n",
    "#     val_df = train_val_df.iloc[val_index]\n",
    "\n",
    "#     # Tokenize the training and validation data\n",
    "#     train_encodings = tokenize_data(train_df['Example Description'].tolist(), tokenizer)\n",
    "#     val_encodings = tokenize_data(val_df['Example Description'].tolist(), tokenizer)\n",
    "\n",
    "#     # Map labels to indices for training and validation\n",
    "#     train_labels = torch.tensor(train_df['Artifact Id'].map(\n",
    "#         lambda x: filtered_labels_list.index(x)).tolist())\n",
    "#     val_labels = torch.tensor(val_df['Artifact Id'].map(\n",
    "#         lambda x: filtered_labels_list.index(x)).tolist())\n",
    "\n",
    "#     # Create TensorDatasets for training and validation\n",
    "#     train_dataset = TensorDataset(\n",
    "#         train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "#     val_dataset = TensorDataset(\n",
    "#         val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "\n",
    "#     # Create DataLoaders for training and validation\n",
    "#     train_loader = DataLoader(train_dataset, sampler=RandomSampler(\n",
    "#         train_dataset), batch_size=batch_size)\n",
    "#     val_loader = DataLoader(val_dataset, sampler=SequentialSampler(\n",
    "#         val_dataset), batch_size=batch_size)\n",
    "\n",
    "#     # Initialize model, optimizer, and scheduler\n",
    "#     model = BertForSequenceClassification.from_pretrained(\n",
    "#         'bert-base-uncased', num_labels=len(filtered_labels_list))\n",
    "#     model.to(device)\n",
    "#     optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "#     total_steps = len(train_loader) * num_epochs\n",
    "#     scheduler = get_linear_schedule_with_warmup(\n",
    "#         optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "#     )\n",
    "\n",
    "#     # Train the model with validation\n",
    "#     accuracy, f1 = train_with_validation(\n",
    "#         model=model,\n",
    "#         train_loader=train_loader,\n",
    "#         val_loader=val_loader,\n",
    "#         optimizer=optimizer,\n",
    "#         scheduler=scheduler,\n",
    "#         device=device,\n",
    "#         num_epochs=num_epochs\n",
    "#     )\n",
    "    \n",
    "#     fold_results.append((accuracy, f1))\n",
    "#     if f1 > best_val_f1:\n",
    "#         best_val_f1 = f1\n",
    "#         best_model = model.state_dict()\n",
    "#  # Best model should be taken in a different way!\n",
    "\n",
    "# # # Average results over all folds\n",
    "# # average_f1 = np.mean([result[0] for result in fold_results])\n",
    "# # average_accuracy = np.mean([result[1] for result in fold_results])\n",
    "# # print(f\"\\n--- K-Fold Results ---\")\n",
    "# # print(f\"Average F1 Score (Weighted): {average_f1:.4f}\")\n",
    "# # print(f\"Average Accuracy: {average_accuracy:.2f}%\")\n",
    "        \n",
    "# test(model, test_loader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for K-Fold Cross Validation\n",
    "best_val_f1 = 0.0\n",
    "fold_results = []  # Store F1 score and accuracy for each fold\n",
    "model_weights = {}  # Store cumulative weights for averaging\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize model weights storage\n",
    "for name, param in BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=len(filtered_labels_list)\n",
    ").named_parameters():\n",
    "    model_weights[name] = torch.zeros_like(param.data)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(train_val_df, train_val_df['Artifact Id'])):\n",
    "    print(f\"\\n--- Fold {fold+1}/{num_folds} ---\")\n",
    "\n",
    "    # Split the train/val set into training and validation data for this fold\n",
    "    train_df = train_val_df.iloc[train_index]\n",
    "    val_df = train_val_df.iloc[val_index]\n",
    "\n",
    "    # Tokenize the training and validation data\n",
    "    train_encodings = tokenize_data(train_df['Example Description'].tolist(), tokenizer)\n",
    "    val_encodings = tokenize_data(val_df['Example Description'].tolist(), tokenizer)\n",
    "\n",
    "    # Map labels to indices for training and validation\n",
    "    train_labels = torch.tensor(train_df['Artifact Id'].map(\n",
    "        lambda x: filtered_labels_list.index(x)).tolist())\n",
    "    val_labels = torch.tensor(val_df['Artifact Id'].map(\n",
    "        lambda x: filtered_labels_list.index(x)).tolist())\n",
    "\n",
    "    # Create TensorDatasets for training and validation\n",
    "    train_dataset = TensorDataset(\n",
    "        train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "    val_dataset = TensorDataset(\n",
    "        val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "\n",
    "    # Create DataLoaders for training and validation\n",
    "    train_loader = DataLoader(train_dataset, sampler=RandomSampler(\n",
    "        train_dataset), batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_dataset, sampler=SequentialSampler(\n",
    "        val_dataset), batch_size=batch_size)\n",
    "\n",
    "    # Initialize model, optimizer, and scheduler\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased', num_labels=len(filtered_labels_list))\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Train the model with validation\n",
    "    accuracy, f1 = train_with_validation(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    fold_results.append((accuracy, f1))\n",
    "    \n",
    "    # Accumulate model weights for weighted average\n",
    "    for name, param in model.named_parameters():\n",
    "        model_weights[name] += param.data.clone() * f1\n",
    "\n",
    "# Calculate weighted average of model weights\n",
    "total_f1_sum = sum([result[1] for result in fold_results])  # Total F1 across folds\n",
    "average_model_state_dict = {}\n",
    "for name, weight in model_weights.items():\n",
    "    average_model_state_dict[name] = weight / total_f1_sum\n",
    "\n",
    "# Load the averaged model weights into a new model\n",
    "average_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', num_labels=len(filtered_labels_list)\n",
    ")\n",
    "average_model.load_state_dict(average_model_state_dict)\n",
    "\n",
    "# Evaluate the averaged model on the test set\n",
    "test(average_model, test_loader, device)\n",
    "\n",
    "# Average results over all folds\n",
    "average_f1 = np.mean([result[1] for result in fold_results])\n",
    "average_accuracy = np.mean([result[0] for result in fold_results])\n",
    "print(f\"\\n--- K-Fold Results ---\")\n",
    "print(f\"Average F1 Score (Weighted): {average_f1:.4f}\")\n",
    "print(f\"Average Accuracy: {average_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = average_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), output_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_dict = torch.load(output_model_path, weights_only=True)\n",
    "\n",
    "_model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    num_labels=len(filtered_labels_list)  \n",
    ")\n",
    "_model.load_state_dict(saved_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All model weights match!\n"
     ]
    }
   ],
   "source": [
    "all_weights_match = True  # Flag to track if all weights match\n",
    "\n",
    "for (name1, param1), (name2, param2) in zip(model.named_parameters(), _model.named_parameters()):\n",
    "    # Check if parameter names match (optional, but useful for debugging)\n",
    "    if name1 != name2:\n",
    "        print(f\"Parameter mismatch: {name1} vs {name2}\")\n",
    "        all_weights_match = False\n",
    "        continue\n",
    "    \n",
    "    # Compare parameter values\n",
    "    if not torch.allclose(param1, param2, atol=1e-6):\n",
    "        print(f\"Mismatch found in parameter: {name1}\")\n",
    "        all_weights_match = False\n",
    "\n",
    "if all_weights_match:\n",
    "    print(\"All model weights match!\")\n",
    "else:\n",
    "    print(\"Some weights do not match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to test_data.csv\n"
     ]
    }
   ],
   "source": [
    "test_df.to_csv('test_data.csv', index=False)\n",
    "print(\"DataFrame saved to test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model on the baseline data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830fc236e28b4b6b83528197e8acdbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.0278\n",
      "Accuracy: 12.50%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4236738aa595460981f63b61f21ad09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.0278\n",
      "Accuracy: 12.50%\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"baseline.csv\"\n",
    "baseline_df = pd.read_csv(csv_path)\n",
    "\n",
    "baseline_encodings = tokenize_data(baseline_df['Example Description'].tolist(), tokenizer)\n",
    "\n",
    "baseline_labels = torch.tensor(baseline_df['Artifact Id'].map(\n",
    "    lambda x: filtered_labels_list.index(x)).tolist())\n",
    "\n",
    "baseline_dataset = TensorDataset(\n",
    "   baseline_encodings['input_ids'], baseline_encodings['attention_mask'], baseline_labels)\n",
    "\n",
    "baseline_loader = DataLoader(baseline_dataset, sampler=RandomSampler(\n",
    "    baseline_dataset), batch_size=batch_size)\n",
    "\n",
    "test(model, baseline_loader, device)\n",
    "test(_model, baseline_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
