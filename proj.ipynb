{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Conda Environment with Python 3.12 and Installing Required Libraries\n",
    "##### Step 1 - Created a Conda Environment with Python 3.12 and Connected to its Kernel\n",
    "I created a new Conda environment with Python 3.12 and activated it. After that, I connected to the Python kernel in Jupyter/VSCode using this environment.\n",
    "\n",
    "Commands to create and activate the environment:\n",
    "```bash\n",
    "conda create -n llm_env python=3.12\n",
    "conda activate llm_env\n",
    "```\n",
    "\n",
    "##### Step 2 - Imported Required Libraries\n",
    "Once the environment was set up and activated, I imported the following libraries for my project:\n",
    "- `numpy` as `np`: For numerical operations and working with arrays.\n",
    "- `pandas` as `pd`: For data manipulation and analysis.\n",
    "- `torch`: PyTorch, for deep learning model building and training.\n",
    "- `tqdm.notebook`: For displaying progress bars in Jupyter notebooks.\n",
    "- `sklearn.preprocessing.LabelEncoder`: For encoding categorical labels into numerical format.\n",
    "- `sklearn.model_selection.train_test_split`: For splitting datasets into training and testing sets.\n",
    "- `transformers.BertTokenizer`: For tokenizing text data for BERT models.\n",
    "- `transformers.BertForSequenceClassification`: For using BERT for sequence classification tasks.\n",
    "- `torch.utils.data.TensorDataset`: For creating datasets from tensors.\n",
    "- `torch.utils.data.DataLoader`: For batching datasets and loading them efficiently during training.\n",
    "- `torch.utils.data.RandomSampler` and `torch.utils.data.SequentialSampler`: For random and sequential sampling of datasets.\n",
    "- `transformers.AdamW` and `transformers.get_linear_schedule_with_warmup`: For the AdamW optimizer and learning rate scheduler used with transformers.\n",
    "- `sklearn.metrics.f1_score`: For evaluating model performance using the F1 score.\n",
    "- `random`: For random number generation, typically used in data shuffling.\n",
    "\n",
    "Commands to install the libraries:\n",
    "```bash\n",
    "conda install numpy pandas scikit-learn tqdm pytorch\n",
    "conda install -c huggingface transformers\n",
    "```\n",
    "\n",
    "Once the libraries were installed, I successfully imported them into the notebook and was ready to proceed with building and training the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "# import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 4\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Handling\n",
    "\n",
    "- **`df`**: The full dataset loaded from the CSV (`dataset.csv`).\n",
    "- **`labels`**: The list of labels (i.e., 'Artifact Id') extracted from the dataset.\n",
    "- **`label_counts`**: The count of occurrences of each unique label in the dataset.\n",
    "- **`filtered_labels`**: The labels that appear at least 5 times in the dataset.\n",
    "- **`filtered_labels_list`**: A list of labels that appear at least 5 times.\n",
    "- **`filtered_df`**: The filtered DataFrame containing only the rows with labels that appear at least 5 times.\n",
    "\n",
    "By running this script, you get a new DataFrame, `filtered_df`, containing only the rows with labels that appear 5 or more times in the original dataset.\n",
    "\n",
    "**Note**: This is a very simple preprocessing step where we filter out labels with fewer than 5 occurrences. There is no further data cleaning, such as handling missing values or data normalization, applied at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# labels = df['Artifact Id']\n",
    "\n",
    "# label_counts = labels.value_counts()\n",
    "\n",
    "# filtered_labels = label_counts[label_counts >= 5]\n",
    "\n",
    "# filtered_labels_list = filtered_labels.index.tolist()\n",
    "\n",
    "# filtered_df = df[df['Artifact Id'].isin(filtered_labels_list)]\n",
    "\n",
    "# print(filtered_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for Overfitting on a Small Portion of the Data\n",
    "\n",
    "In this part of the code, we filter the dataset to only include labels that have exactly **16 samples**. By training the model on this small subset, we aim to observe if it achieves **overfitting**, where it memorizes the data rather than generalizing well. This helps us test the model's ability to avoid overfitting on limited data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# labels = df['Artifact Id']\n",
    "\n",
    "# label_counts = labels.value_counts()\n",
    "\n",
    "# filtered_labels = label_counts[label_counts == 16]\n",
    "\n",
    "# filtered_labels_list = filtered_labels.index.tolist()\n",
    "\n",
    "# filtered_df = df[df['Artifact Id'].isin(filtered_labels_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the Dataset\n",
    "\n",
    "To address class imbalance in the dataset, we performed the following steps:\n",
    "\n",
    "1. **Random Sampling of 'Command' Class**: We noticed that the **'d3f:Command'** class was underrepresented in the dataset. Therefore, we **randomly sampled 16 instances** from this class to ensure it is adequately represented.\n",
    "\n",
    "2. **Combining the Data**: After sampling the 'Command' class, we combined it with the filtered dataset to create a **more balanced dataset**.\n",
    "\n",
    "This process ensures that the model is not biased towards the larger classes and helps it generalize better across all classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact Id\n",
      "d3f:Database                     16\n",
      "d3f:Software                     16\n",
      "d3f:Command                      16\n",
      "d3f:HardwareDriver               14\n",
      "d3f:DisplayServer                11\n",
      "d3f:OperatingSystem               8\n",
      "d3f:FileSystem                    7\n",
      "d3f:BootLoader                    6\n",
      "d3f:InterprocessCommunication     5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "labels = df['Artifact Id']\n",
    "\n",
    "label_counts = labels.value_counts()\n",
    "\n",
    "filtered_labels = label_counts[(label_counts >= 5) & (label_counts <= 200)] # Remove \"Command\" label\n",
    "\n",
    "filtered_labels_list = filtered_labels.index.tolist()\n",
    "\n",
    "filtered_df = df[df['Artifact Id'].isin(filtered_labels_list)]\n",
    "\n",
    "command_df = df[df['Artifact Id'] == 'd3f:Command']\n",
    "sampled_command_df = command_df.sample(n=16, random_state=42)\n",
    "combined_df = pd.concat([filtered_df, sampled_command_df])\n",
    "\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(combined_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide to Train, Validation, and Test While Keeping the Distribution\n",
    "\n",
    "In this step, the dataset is split into a training set, a validation set, and a testing set, while maintaining the same label distribution across all sets. This is done using the `train_test_split` function from `sklearn.model_selection`, with the `stratify` parameter set to the labels (`Artifact Id`) to ensure that all splits preserve the same class proportions as the original dataset.\n",
    "\n",
    "- **Training Set**: Contains 64% of the data.\n",
    "- **Validation Set**: Contains 16% of the data.\n",
    "- **Test Set**: Contains 20% of the data.\n",
    "\n",
    "By using stratified splitting, the label distribution in all sets (training, validation, and test) is consistent, preventing potential bias caused by imbalanced labels in smaller datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(filtered_df,\n",
    "                                     test_size=0.2,\n",
    "                                     stratify=filtered_df['Artifact Id'],\n",
    "                                     random_state=42)\n",
    "\n",
    "train_df, val_df = train_test_split(train_df,\n",
    "                                    test_size=0.2,\n",
    "                                    stratify=train_df['Artifact Id'],\n",
    "                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set label distribution:\n",
      "Artifact Id\n",
      "d3f:Software                     10\n",
      "d3f:Database                     10\n",
      "d3f:HardwareDriver                9\n",
      "d3f:DisplayServer                 7\n",
      "d3f:OperatingSystem               5\n",
      "d3f:BootLoader                    4\n",
      "d3f:FileSystem                    4\n",
      "d3f:InterprocessCommunication     3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set label distribution:\")\n",
    "print(train_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nValidation set label distribution:\")\n",
    "# print(val_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set label distribution:\n",
      "Artifact Id\n",
      "d3f:Software                     3\n",
      "d3f:Database                     3\n",
      "d3f:HardwareDriver               3\n",
      "d3f:OperatingSystem              2\n",
      "d3f:DisplayServer                2\n",
      "d3f:FileSystem                   2\n",
      "d3f:InterprocessCommunication    1\n",
      "d3f:BootLoader                   1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest set label distribution:\")\n",
    "print(test_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Tokenizer Initialization\n",
    "\n",
    "- **Model**: A pre-trained `BertForSequenceClassification` model is loaded from `bert-base-uncased`, with a classification head set to `num_labels` (number of unique labels in the dataset).\n",
    "\n",
    "- **Tokenizer**: The `BertTokenizer` for `bert-base-uncased` is used for consistent tokenization.\n",
    "\n",
    "These components enable fine-tuning of the BERT model for the multiclass classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(filtered_labels_list))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation: Tokenization and Dataloader Creation\n",
    "\n",
    "- **`tokenize_data`**: Tokenizes input text using the BERT tokenizer, padding and truncating to a max length of 512 tokens, and returns PyTorch tensors.\n",
    "\n",
    "- **Tokenization**: The training, validation, and test data (`'Example Description'`) are tokenized into input IDs and attention masks.\n",
    "\n",
    "- **Label Encoding**: Labels (`'Artifact Id'`) are mapped to indices and converted to tensor labels for all datasets.\n",
    "\n",
    "- **Dataset Creation**: `TensorDataset` objects are created for training, validation, and test data with tokenized inputs and labels.\n",
    "\n",
    "- **DataLoader**: Three DataLoaders are created:\n",
    "  - `train_loader`: Random sampling for training.\n",
    "  - `val_loader` and `test_loader`: Sequential sampling for validation and testing, with a batch size of 16.\n",
    "\n",
    "These steps prepare the data for efficient use in training, validation, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "# Tokenize the training, validation, and test data\n",
    "train_encodings = tokenize_data(train_df['Example Description'].tolist())\n",
    "val_encodings = tokenize_data(val_df['Example Description'].tolist())  # Validation set\n",
    "test_encodings = tokenize_data(test_df['Example Description'].tolist())\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_df['Artifact Id'].map(lambda x: filtered_labels_list.index(x)).tolist())\n",
    "val_labels = torch.tensor(val_df['Artifact Id'].map(lambda x: filtered_labels_list.index(x)).tolist())  # Validation set\n",
    "test_labels = torch.tensor(test_df['Artifact Id'].map(lambda x: filtered_labels_list.index(x)).tolist())\n",
    "\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)  # Validation set\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)  # Validation set\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Learning Rate Scheduler\n",
    "\n",
    "- **Optimizer**: \n",
    "  - `AdamW` is used, suitable for fine-tuning transformer models like BERT, with a learning rate of `5e-5`.\n",
    "  \n",
    "- **Learning Rate Scheduler**: \n",
    "  - A linear scheduler adjusts the learning rate over `total_steps` (batches × epochs), with no warm-up phase (`num_warmup_steps=0`).\n",
    "\n",
    "These components ensure efficient model optimization during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function\n",
    "\n",
    "This function trains the model for a specified number of epochs:\n",
    "\n",
    "1. **Forward Pass**: For each batch, the model computes the loss and logits, then makes predictions by selecting the class with the highest logit.\n",
    "2. **Metrics Calculation**: It tracks the total loss, accuracy, and F1 score across all batches in each epoch. Accuracy is calculated by comparing the predicted labels to the true labels, and the F1 score is computed using the weighted average across all classes.\n",
    "3. **Optimization**: The model parameters are updated using backpropagation (`loss.backward()`), and the optimizer and learning rate scheduler are updated each step.\n",
    "\n",
    "At the end of each epoch, the loss, accuracy, and F1 score are printed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, scheduler, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Store predictions and labels for F1 score calculation\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "            # Accuracy calculation\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate metrics for the epoch\n",
    "        accuracy = total_correct / total_samples * 100\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Validation\n",
    "\n",
    "The `train_with_validation` function trains the model for multiple epochs and evaluates it on a validation set after each epoch. \n",
    "\n",
    "1. **Training**: For each batch, the model computes the loss, updates parameters, and tracks accuracy and F1 score.\n",
    "2. **Validation**: After each epoch, the model's performance is evaluated on the validation set, and validation accuracy and F1 score are calculated.\n",
    "\n",
    "Metrics (training and validation accuracy, F1 score) are printed after each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_validation(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "            correct = (preds == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate training metrics\n",
    "        accuracy = total_correct / total_samples * 100\n",
    "        f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "        val_all_labels = []\n",
    "        val_all_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}/{num_epochs}\"):\n",
    "                input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                val_all_preds.extend(preds.cpu().tolist())\n",
    "                val_all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "                correct = (preds == labels).sum().item()\n",
    "                val_correct += correct\n",
    "                val_samples += labels.size(0)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_accuracy = val_correct / val_samples * 100\n",
    "        val_f1 = f1_score(val_all_labels, val_all_preds, average=\"weighted\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.6f}, Accuracy: {accuracy:.6f}%, F1 Score: {f1:.6f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.6f}%, Validation F1 Score: {val_f1:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0bcfefe3994e91bdfb1aaa5285a5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1/4:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[285], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train(\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#     model=model,\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     train_loader=train_loader,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#     num_epochs=num_epochs\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m train_with_validation(\n\u001b[0;32m     11\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     12\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m     13\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m     14\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     15\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[0;32m     16\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     17\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs\n\u001b[0;32m     18\u001b[0m )\n",
      "Cell \u001b[1;32mIn[284], line 28\u001b[0m, in \u001b[0;36mtrain_with_validation\u001b[1;34m(model, train_loader, val_loader, optimizer, scheduler, device, num_epochs)\u001b[0m\n\u001b[0;32m     25\u001b[0m total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m correct\n\u001b[0;32m     26\u001b[0m total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     optimizer=optimizer,\n",
    "#     scheduler=scheduler,\n",
    "#     device=device,\n",
    "#     num_epochs=num_epochs\n",
    "# )\n",
    "\n",
    "train_with_validation(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "After training, the model is evaluated on the test set to assess its performance on unseen data. During this phase, the model is set to evaluation mode using `model.eval()`, and predictions are made on the test set.\n",
    "\n",
    "- **Prediction**: For each batch in the test set, the model generates predictions (logits), which are converted to class labels using `torch.argmax()`.\n",
    "  \n",
    "- **F1 Score**: The F1 score, calculated using the `f1_score` function from `sklearn.metrics`, is reported to measure the weighted average performance across all classes.\n",
    "  \n",
    "- **Accuracy**: The accuracy of the model is calculated by comparing the predicted labels to the true labels and computing the percentage of correct predictions.\n",
    "\n",
    "The evaluation results help in understanding the model's effectiveness and generalization to the test set.\n",
    "\n",
    "*Note: This evaluation process does not update the model parameters, ensuring that it is a true test of performance.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate F1 score (weighted) for multiclass classification\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.sum(np.array(predictions) == np.array(true_labels)) / len(true_labels) * 100\n",
    "\n",
    "    print(f\"F1 Score (Weighted): {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16c801794ee49d8bccfb00f2219ee35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.1647\n",
      "Accuracy: 29.41%\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
