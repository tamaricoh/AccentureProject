{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Conda Environment with Python 3.12 and Installing Required Libraries\n",
    "##### Step 1 - Created a Conda Environment with Python 3.12 and Connected to its Kernel\n",
    "I created a new Conda environment with Python 3.12 and activated it. After that, I connected to the Python kernel in Jupyter/VSCode using this environment.\n",
    "\n",
    "Commands to create and activate the environment:\n",
    "```bash\n",
    "conda create -n llm_env python=3.12\n",
    "conda activate llm_env\n",
    "```\n",
    "\n",
    "##### Step 2 - Imported Required Libraries\n",
    "Once the environment was set up and activated, I imported the following libraries for my project:\n",
    "- `numpy` as `np`: For numerical operations and working with arrays.\n",
    "- `pandas` as `pd`: For data manipulation and analysis.\n",
    "- `torch`: PyTorch, for deep learning model building and training.\n",
    "- `tqdm.notebook`: For displaying progress bars in Jupyter notebooks.\n",
    "- `sklearn.preprocessing.LabelEncoder`: For encoding categorical labels into numerical format.\n",
    "- `sklearn.model_selection.train_test_split`: For splitting datasets into training and testing sets.\n",
    "- `transformers.BertTokenizer`: For tokenizing text data for BERT models.\n",
    "- `transformers.BertForSequenceClassification`: For using BERT for sequence classification tasks.\n",
    "- `torch.utils.data.TensorDataset`: For creating datasets from tensors.\n",
    "- `torch.utils.data.DataLoader`: For batching datasets and loading them efficiently during training.\n",
    "- `torch.utils.data.RandomSampler` and `torch.utils.data.SequentialSampler`: For random and sequential sampling of datasets.\n",
    "- `transformers.AdamW` and `transformers.get_linear_schedule_with_warmup`: For the AdamW optimizer and learning rate scheduler used with transformers.\n",
    "- `sklearn.metrics.f1_score`: For evaluating model performance using the F1 score.\n",
    "- `random`: For random number generation, typically used in data shuffling.\n",
    "\n",
    "Commands to install the libraries:\n",
    "```bash\n",
    "conda install numpy pandas scikit-learn tqdm pytorch\n",
    "conda install -c huggingface transformers\n",
    "```\n",
    "\n",
    "Once the libraries were installed, I successfully imported them into the notebook and was ready to proceed with building and training the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "# import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 8\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate = 5e-5 â†’ Achieved ~77% to ~79% Accuracy on both the Test Set and the Train Set by the **Third** Epoch\n",
    "\n",
    "This was the learning rate that gave the best results among the following options - 5e-3, 3e-5, 2e-5, 3e-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Handling\n",
    "\n",
    "- **`df`**: The full dataset loaded from the CSV (`dataset.csv`).\n",
    "- **`labels`**: The list of labels (i.e., 'Artifact Id') extracted from the dataset.\n",
    "- **`label_counts`**: The count of occurrences of each unique label in the dataset.\n",
    "- **`filtered_labels`**: The labels that appear at least 5 times in the dataset.\n",
    "- **`filtered_labels_list`**: A list of labels that appear at least 5 times.\n",
    "- **`filtered_df`**: The filtered DataFrame containing only the rows with labels that appear at least 5 times.\n",
    "\n",
    "By running this script, you get a new DataFrame, `filtered_df`, containing only the rows with labels that appear 5 or more times in the original dataset.\n",
    "\n",
    "**Note**: This is a very simple preprocessing step where we filter out labels with fewer than 5 occurrences. There is no further data cleaning, such as handling missing values or data normalization, applied at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact Id\n",
      "d3f:Command                      250\n",
      "d3f:Database                      16\n",
      "d3f:Software                      16\n",
      "d3f:HardwareDriver                14\n",
      "d3f:DisplayServer                 11\n",
      "d3f:OperatingSystem                8\n",
      "d3f:FileSystem                     7\n",
      "d3f:BootLoader                     6\n",
      "d3f:InterprocessCommunication      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "labels = df['Artifact Id']\n",
    "\n",
    "label_counts = labels.value_counts()\n",
    "\n",
    "filtered_labels = label_counts[label_counts >= 5]\n",
    "\n",
    "filtered_labels_list = filtered_labels.index.tolist()\n",
    "\n",
    "filtered_df = df[df['Artifact Id'].isin(filtered_labels_list)]\n",
    "\n",
    "print(filtered_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if my model can reach overfitting on a small dataset with two labels\n",
    "\n",
    "I tested whether the model trains well by taking a small data subset to induce overfitting. Indeed, by the third epoch, I achieved 96% accuracy on the train set compared to 57% on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# labels = df['Artifact Id']\n",
    "\n",
    "# label_counts = labels.value_counts()\n",
    "\n",
    "# filtered_labels = label_counts[label_counts == 16]\n",
    "\n",
    "# filtered_labels_list = filtered_labels.index.tolist()\n",
    "\n",
    "# filtered_df = df[df['Artifact Id'].isin(filtered_labels_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# labels = df['Artifact Id']\n",
    "\n",
    "# label_counts = labels.value_counts()\n",
    "\n",
    "# filtered_labels = label_counts[(label_counts >= 5) & (label_counts <= 200)]\n",
    "\n",
    "# filtered_labels_list = filtered_labels.index.tolist()\n",
    "\n",
    "# filtered_df = df[df['Artifact Id'].isin(filtered_labels_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide to Train, Validation, and Test While Keeping the Distribution\n",
    "\n",
    "In this step, the dataset is split into a training set, a validation set, and a testing set, while maintaining the same label distribution across all sets. This is done using the `train_test_split` function from `sklearn.model_selection`, with the `stratify` parameter set to the labels (`Artifact Id`) to ensure that all splits preserve the same class proportions as the original dataset.\n",
    "\n",
    "- **Training Set**: Contains 64% of the data.\n",
    "- **Validation Set**: Contains 16% of the data.\n",
    "- **Test Set**: Contains 20% of the data.\n",
    "\n",
    "By using stratified splitting, the label distribution in all sets (training, validation, and test) is consistent, preventing potential bias caused by imbalanced labels in smaller datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(filtered_df,\n",
    "                                     test_size=0.2,\n",
    "                                     stratify=filtered_df['Artifact Id'],\n",
    "                                     random_state=42)\n",
    "\n",
    "train_df, val_df = train_test_split(train_df,\n",
    "                                    test_size=0.2,\n",
    "                                    stratify=train_df['Artifact Id'],\n",
    "                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set label distribution:\n",
      "Artifact Id\n",
      "d3f:Command                      160\n",
      "d3f:Software                      10\n",
      "d3f:Database                      10\n",
      "d3f:HardwareDriver                 9\n",
      "d3f:DisplayServer                  7\n",
      "d3f:OperatingSystem                5\n",
      "d3f:FileSystem                     4\n",
      "d3f:BootLoader                     4\n",
      "d3f:InterprocessCommunication      3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set label distribution:\")\n",
    "print(train_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set label distribution:\n",
      "Artifact Id\n",
      "d3f:Command                      40\n",
      "d3f:Database                      3\n",
      "d3f:Software                      3\n",
      "d3f:HardwareDriver                2\n",
      "d3f:DisplayServer                 2\n",
      "d3f:InterprocessCommunication     1\n",
      "d3f:OperatingSystem               1\n",
      "d3f:FileSystem                    1\n",
      "d3f:BootLoader                    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nValidation set label distribution:\")\n",
    "print(val_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set label distribution:\n",
      "Artifact Id\n",
      "d3f:Command                      50\n",
      "d3f:HardwareDriver                3\n",
      "d3f:Software                      3\n",
      "d3f:Database                      3\n",
      "d3f:DisplayServer                 2\n",
      "d3f:FileSystem                    2\n",
      "d3f:OperatingSystem               2\n",
      "d3f:BootLoader                    1\n",
      "d3f:InterprocessCommunication     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest set label distribution:\")\n",
    "print(test_df['Artifact Id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Tokenizer Initialization\n",
    "\n",
    "- **Model**:\n",
    "  - A pre-trained `BertForSequenceClassification` model is loaded from the `bert-base-uncased` variant of BERT.\n",
    "  - The model includes a classification head configured with `num_labels`, which corresponds to the number of unique labels in the filtered dataset (`filtered_labels_list`).\n",
    "\n",
    "- **Tokenizer**:\n",
    "  - The `BertTokenizer` associated with `bert-base-uncased` is initialized for consistent tokenization of input text.\n",
    "\n",
    "These components leverage the pre-trained BERT architecture for fine-tuning on the specific multiclass classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(filtered_labels_list))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation: Tokenization and Dataloader Creation\n",
    "\n",
    "- **`tokenize_data` Function**: A helper function that tokenizes input text using the BERT tokenizer. It ensures the data is padded, truncated to a maximum length of 512 tokens, and returned as PyTorch tensors.\n",
    "\n",
    "- **Tokenization**:\n",
    "  - The training, validation, and test data (`'Example Description'`) are tokenized into input IDs and attention masks using the `tokenize_data` function.\n",
    "\n",
    "- **Label Encoding**:\n",
    "  - Labels (`'Artifact Id'`) are mapped to their index positions in the `filtered_labels_list` to create tensor labels for the training, validation, and test datasets.\n",
    "\n",
    "- **Dataset Creation**:\n",
    "  - Training, validation, and test data are combined into `TensorDataset` objects, including tokenized inputs (`input_ids`, `attention_mask`) and the corresponding labels.\n",
    "\n",
    "- **DataLoader**:\n",
    "  - `train_loader`: A DataLoader with a random sampling strategy for shuffling and a batch size of 16.\n",
    "  - `val_loader`: A DataLoader with a sequential sampling strategy for evaluation and the same batch size, used for validation.\n",
    "  - `test_loader`: A DataLoader with a sequential sampling strategy for evaluation and the same batch size.\n",
    "\n",
    "These steps prepare the tokenized data and labels for efficient use during training, validation, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "# Tokenize the training, validation, and test data\n",
    "train_encodings = tokenize_data(train_df['Example Description'].tolist())\n",
    "val_encodings = tokenize_data(val_df['Example Description'].tolist())  # Validation set\n",
    "test_encodings = tokenize_data(test_df['Example Description'].tolist())\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_df['Artifact Id'].map(lambda x: filtered_labels_list.index(x)).tolist())\n",
    "val_labels = torch.tensor(val_df['Artifact Id'].map(lambda x: filtered_labels_list.index(x)).tolist())  # Validation set\n",
    "test_labels = torch.tensor(test_df['Artifact Id'].map(lambda x: filtered_labels_list.index(x)).tolist())\n",
    "\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)  # Validation set\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)  # Validation set\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Learning Rate Scheduler\n",
    "\n",
    "- **Optimizer**:\n",
    "  - `AdamW` is used as the optimizer, designed for fine-tuning transformer-based models like BERT.\n",
    "  - The learning rate is set to `learning_rate` (e.g., `5e-5`), which is critical for stable training.\n",
    "\n",
    "- **Learning Rate Scheduler**:\n",
    "  - A linear scheduler with a warm-up phase (`num_warmup_steps=0`) is employed.\n",
    "  - The scheduler adjusts the learning rate gradually over `total_steps` (calculated as `number of batches Ã— number of epochs`).\n",
    "\n",
    "These components ensure efficient and stable optimization of the model parameters during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Evaluation\n",
    "\n",
    "In this part of the process, the model is trained for a specified number of epochs, with performance monitored by calculating both the training loss and accuracy.\n",
    "\n",
    "- **Training Phase**: The model is trained using batches of data, with the optimizer and scheduler being updated accordingly. For each batch, the loss is calculated and backpropagated to adjust the model's parameters. Training accuracy is calculated based on correct predictions.\n",
    "\n",
    "Finally, the training accuracies and losses over the epochs are plotted to visualize the model's performance.\n",
    "\n",
    "This process helps track the model's training progress, ensuring it learns effectively from the data.\n",
    "\n",
    "---\n",
    "\n",
    "*Note: The graphs display training metrics, helping to assess the modelâ€™s learning efficiency.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc17d1db8def49fc8264d93330a96d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1/8:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5569, Accuracy: 58.49%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673e2fc93aa6463e8c02c3ad9cc2ad52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2/8:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.9137, Accuracy: 75.47%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2665a76524747f792ae51cd1e3c3b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3/8:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.7610, Accuracy: 75.47%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d07fd2b69f747bdbc9a3e0029e9fbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4/8:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.5999, Accuracy: 80.66%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ce2fe1680449cca76cd56fc7f36842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5/8:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.5277, Accuracy: 81.13%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e181e907cbb1478289e0ab3bf54b34ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6/8:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.4531, Accuracy: 87.26%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703570cdb26c4c56a567b8b9101aa2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7/8:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.4205, Accuracy: 88.68%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883ae1dbf92e4d97ac35bf9f837af84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8/8:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.4162, Accuracy: 91.04%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        correct = (preds == labels).sum().item()\n",
    "        total_correct += correct\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate and print accuracy for the epoch\n",
    "    accuracy = total_correct / total_samples * 100\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "After training, the model is evaluated on the test set to assess its performance on unseen data. During this phase, the model is set to evaluation mode using `model.eval()`, and predictions are made on the test set.\n",
    "\n",
    "- **Prediction**: For each batch in the test set, the model generates predictions (logits), which are converted to class labels using `torch.argmax()`.\n",
    "  \n",
    "- **F1 Score**: The F1 score, calculated using the `f1_score` function from `sklearn.metrics`, is reported to measure the weighted average performance across all classes.\n",
    "  \n",
    "- **Accuracy**: The accuracy of the model is calculated by comparing the predicted labels to the true labels and computing the percentage of correct predictions.\n",
    "\n",
    "The evaluation results help in understanding the model's effectiveness and generalization to the test set.\n",
    "\n",
    "*Note: This evaluation process does not update the model parameters, ensuring that it is a true test of performance.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738a6db2c4a3439697fe227a5e85730f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.8430827830335148\n",
      "Accuracy: 88.06%\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate the F1 score for multiclass classification\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "print(f\"F1 Score (Weighted): {f1}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.sum(np.array(predictions) == np.array(true_labels)) / len(true_labels) * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
