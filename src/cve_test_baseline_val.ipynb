{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from pys.functions import CustomBertModel, create_dataset, train_with_validation, test\n",
    "from pys.params import batch_size, learning_rate, num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv('../csv/dataset.csv')\n",
    "baseline_df = pd.read_csv('../csv/baseline.csv')\n",
    "dataset_aug_df = pd.read_csv('../csv/dataset_aug.csv')\n",
    "test_df = pd.read_csv('../csv/cve_data.csv')\n",
    "\n",
    "baseline_descriptions = baseline_df['Example Description'].tolist()\n",
    "\n",
    "filtered_dataset_df = dataset_df[~dataset_df['Example Description'].isin(\n",
    "    baseline_descriptions)]\n",
    "\n",
    "train_df = pd.concat([filtered_dataset_df, dataset_aug_df], ignore_index=True)\n",
    "val_df = baseline_df.copy()\n",
    "test_df = test_df.copy()\n",
    "\n",
    "train_labels = train_df['Artifact Id']\n",
    "val_labels = val_df['Artifact Id']\n",
    "test_labels = test_df['Artifact Id']\n",
    "\n",
    "train_labels_counts = train_labels.value_counts()\n",
    "val_labels_counts = val_labels.value_counts()\n",
    "test_labels_counts = test_labels.value_counts()\n",
    "\n",
    "all_unique_labels = list(set(train_labels.unique().tolist(\n",
    ") + val_labels.unique().tolist() + test_labels.unique().tolist()))\n",
    "\n",
    "label_mapping = {label: idx for idx, label in enumerate(all_unique_labels)}\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_max = 50\n",
    "eta_min = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout Layer: bert.bert.embeddings.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.0.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.0.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.0.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.1.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.1.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.1.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.2.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.2.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.2.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.3.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.3.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.3.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.4.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.4.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.4.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.5.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.5.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.5.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.6.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.6.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.6.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.7.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.7.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.7.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.8.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.8.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.8.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.9.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.9.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.9.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.10.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.10.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.10.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.11.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.11.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.11.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.dropout, Dropout Probability: 0.1\n",
      "0.1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CustomBertModel' object has no attribute 'dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropout Layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Dropout Probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mdropout\u001b[38;5;241m.\u001b[39mp)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mdropout\u001b[38;5;241m.\u001b[39mp)\n\u001b[0;32m     18\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, sampler\u001b[38;5;241m=\u001b[39mRandomSampler(train_dataset), batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m     19\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, sampler\u001b[38;5;241m=\u001b[39mRandomSampler(val_dataset), batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[1;32mc:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CustomBertModel' object has no attribute 'dropout'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = create_dataset(train_df, tokenizer, label_mapping)\n",
    "val_dataset = create_dataset(val_df, tokenizer, label_mapping)\n",
    "test_dataset = create_dataset(test_df, tokenizer, label_mapping)\n",
    "\n",
    "model = CustomBertModel(num_labels=len(label_mapping))\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, sampler=RandomSampler(val_dataset), batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
    "\n",
    "f1_train, f1_val, acc_train, acc_val, loss_train, loss_val = train_with_validation(\n",
    "    model, train_loader, val_loader, optimizer, device, num_epochs, scheduler\n",
    ")\n",
    "\n",
    "f1_test, acc_test = test(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training results\n",
    "print(\"\\n--- Training Results ---\")\n",
    "print(f\"Training F1 Score (Weighted): {f1_train[-1]:.4f}\")\n",
    "print(f\"Training Accuracy: {acc_train[-1]:.2f}%\")\n",
    "print(f\"Training Loss: {loss_train[-1]:.4f}\")\n",
    "\n",
    "# Print validation results\n",
    "print(\"\\n--- Validation Results ---\")\n",
    "print(f\"Validation F1 Score (Weighted): {f1_val[-1]:.4f}\")\n",
    "print(f\"Validation Accuracy: {acc_val[-1]:.2f}%\")\n",
    "print(f\"Validation Loss: {loss_val[-1]:.4f}\")\n",
    "\n",
    "# Print test results\n",
    "print(\"\\n--- Test Results ---\")\n",
    "print(f\"Test F1 Score (Weighted): {f1_test:.4f}\")\n",
    "print(f\"Test Accuracy: {acc_test:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
