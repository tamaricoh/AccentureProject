{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from pys.functions import CustomBertModel, create_dataset, train_with_validation, test\n",
    "from pys.cve_test_baseline_val_data import train_df, val_df, test_df, label_mapping\n",
    "from pys.params import batch_size, learning_rate, num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_max = 50\n",
    "eta_min = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout Layer: bert.bert.embeddings.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.0.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.0.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.0.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.1.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.1.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.1.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.2.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.2.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.2.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.3.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.3.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.3.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.4.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.4.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.4.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.5.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.5.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.5.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.6.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.6.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.6.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.7.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.7.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.7.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.8.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.8.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.8.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.9.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.9.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.9.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.10.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.10.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.10.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.11.attention.self.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.11.attention.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.bert.encoder.layer.11.output.dropout, Dropout Probability: 0.1\n",
      "Dropout Layer: bert.dropout, Dropout Probability: 0.1\n",
      "0.1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CustomBertModel' object has no attribute 'dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropout Layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Dropout Probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mdropout\u001b[38;5;241m.\u001b[39mp)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mdropout\u001b[38;5;241m.\u001b[39mp)\n\u001b[0;32m     18\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, sampler\u001b[38;5;241m=\u001b[39mRandomSampler(train_dataset), batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m     19\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, sampler\u001b[38;5;241m=\u001b[39mRandomSampler(val_dataset), batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[1;32mc:\\Users\\tamar\\anaconda3\\envs\\llm_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CustomBertModel' object has no attribute 'dropout'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = create_dataset(train_df, tokenizer, label_mapping)\n",
    "val_dataset = create_dataset(val_df, tokenizer, label_mapping)\n",
    "test_dataset = create_dataset(test_df, tokenizer, label_mapping)\n",
    "\n",
    "model = CustomBertModel(num_labels=len(label_mapping))\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
    "model.to(device)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Dropout):\n",
    "        print(f\"Dropout Layer: {name}, Dropout Probability: {module.p}\")\n",
    "print(model.bert.bert.embeddings.dropout.p)\n",
    "print(model.dropout.p)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, sampler=RandomSampler(val_dataset), batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
    "\n",
    "# f1_train, f1_val, acc_train, acc_val, loss_train, loss_val = train_with_validation(\n",
    "#     model, train_loader, val_loader, optimizer, device, num_epochs, scheduler\n",
    "# )\n",
    "\n",
    "# f1_test, acc_test = test(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training results\n",
    "print(\"\\n--- Training Results ---\")\n",
    "print(f\"Training F1 Score (Weighted): {f1_train[-1]:.4f}\")\n",
    "print(f\"Training Accuracy: {acc_train[-1]:.2f}%\")\n",
    "print(f\"Training Loss: {loss_train[-1]:.4f}\")\n",
    "\n",
    "# Print validation results\n",
    "print(\"\\n--- Validation Results ---\")\n",
    "print(f\"Validation F1 Score (Weighted): {f1_val[-1]:.4f}\")\n",
    "print(f\"Validation Accuracy: {acc_val[-1]:.2f}%\")\n",
    "print(f\"Validation Loss: {loss_val[-1]:.4f}\")\n",
    "\n",
    "# Print test results\n",
    "print(\"\\n--- Test Results ---\")\n",
    "print(f\"Test F1 Score (Weighted): {f1_test:.4f}\")\n",
    "print(f\"Test Accuracy: {acc_test:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
