{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR, MultiStepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "from pys.functions import train_with_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - Instead of using grid search for hyperparameter tuning of the learning rate schedulers, Bayesian Optimization can be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_lr(model, train_loader, val_loader, optimizer, device, num_epochs):\n",
    "\n",
    "    grid_params_ExponentialLR = {\n",
    "        'gamma': [0.9]\n",
    "    }\n",
    "\n",
    "    best_f1 = float('-inf')\n",
    "    best_f1_train = None\n",
    "    best_f1_val = None\n",
    "    best_acc_train = None\n",
    "    best_acc_val = None\n",
    "    best_loss_train = None\n",
    "    best_loss_val = None\n",
    "    best_scheduler = None\n",
    "    best_scheduler_params = None\n",
    "\n",
    "    for gamma in grid_params_ExponentialLR['gamma']:\n",
    "        scheduler = ExponentialLR(optimizer, gamma=gamma)\n",
    "        \n",
    "        print(f\"Using ExponentialLR with gamma={gamma}\")\n",
    "        f1_train, f1_val, acc_train, acc_val, loss_train, loss_val = train_with_validation(\n",
    "            model, train_loader, val_loader, optimizer, device, num_epochs, scheduler\n",
    "        )\n",
    "\n",
    "        if f1_val[-1] > best_f1:\n",
    "            best_f1 = f1_val[-1]\n",
    "            best_f1_train = f1_train\n",
    "            best_f1_val = f1_val\n",
    "            best_acc_train = acc_train\n",
    "            best_acc_val = acc_val\n",
    "            best_loss_train = loss_train\n",
    "            best_loss_val = loss_val\n",
    "            best_scheduler = scheduler\n",
    "            best_scheduler_params = {'gamma': gamma}\n",
    "\n",
    "    return best_f1_train, best_f1_val, best_acc_train, best_acc_val, best_loss_train, best_loss_val, best_scheduler, best_scheduler_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_lr(model, train_loader, val_loader, optimizer, device, num_epochs):\n",
    "    grid_params_StepLR = {\n",
    "        'step_size': [2],\n",
    "        'gamma': [0.9]\n",
    "    }\n",
    "\n",
    "    best_f1 = float('-inf')\n",
    "    best_f1_train = None\n",
    "    best_f1_val = None\n",
    "    best_acc_train = None\n",
    "    best_acc_val = None\n",
    "    best_loss_train = None\n",
    "    best_loss_val = None\n",
    "    best_scheduler = None\n",
    "    best_scheduler_params = None\n",
    "\n",
    "    for step_size in grid_params_StepLR['step_size']:\n",
    "        for gamma in grid_params_StepLR['gamma']:\n",
    "            scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "            \n",
    "            print(f\"Using StepLR with step_size={step_size} and gamma={gamma}\")\n",
    "            f1_train, f1_val, acc_train, acc_val, loss_train, loss_val = train_with_validation(\n",
    "                model, train_loader, val_loader, optimizer, device, num_epochs, scheduler\n",
    "            )\n",
    "\n",
    "            if f1_val[-1] > best_f1:\n",
    "                best_f1 = f1_val[-1]\n",
    "                best_f1_train = f1_train\n",
    "                best_f1_val = f1_val\n",
    "                best_acc_train = acc_train\n",
    "                best_acc_val = acc_val\n",
    "                best_loss_train = loss_train\n",
    "                best_loss_val = loss_val\n",
    "                best_scheduler = scheduler\n",
    "                best_scheduler_params = {'step_size': step_size, 'gamma': gamma}\n",
    "\n",
    "    return best_f1_train, best_f1_val, best_acc_train, best_acc_val, best_loss_train, best_loss_val, best_scheduler, best_scheduler_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_lr(model, train_loader, val_loader, optimizer, device, num_epochs):    \n",
    "    grid_params_MultiStepLR = {\n",
    "        'milestones': [[1, 5]],\n",
    "        'gamma': [0.1]\n",
    "    }\n",
    "\n",
    "    best_f1 = float('-inf')\n",
    "    best_f1_train = None\n",
    "    best_f1_val = None\n",
    "    best_acc_train = None\n",
    "    best_acc_val = None\n",
    "    best_loss_train = None\n",
    "    best_loss_val = None\n",
    "    best_scheduler = None\n",
    "    best_scheduler_params = None\n",
    "\n",
    "    for milestones in grid_params_MultiStepLR['milestones']:\n",
    "        for gamma in grid_params_MultiStepLR['gamma']:\n",
    "            scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "            \n",
    "            print(f\"Using MultiStepLR with milestones={milestones} and gamma={gamma}\")\n",
    "            f1_train, f1_val, acc_train, acc_val, loss_train, loss_val = train_with_validation(\n",
    "                model, train_loader, val_loader, optimizer, device, num_epochs, scheduler\n",
    "            )\n",
    "\n",
    "            if f1_val[-1] > best_f1:\n",
    "                best_f1 = f1_val[-1]\n",
    "                best_f1_train = f1_train\n",
    "                best_f1_val = f1_val\n",
    "                best_acc_train = acc_train\n",
    "                best_acc_val = acc_val\n",
    "                best_loss_train = loss_train\n",
    "                best_loss_val = loss_val\n",
    "                best_scheduler = scheduler\n",
    "                best_scheduler_params = {'milestones': milestones, 'gamma': gamma}\n",
    "\n",
    "    return best_f1_train, best_f1_val, best_acc_train, best_acc_val, best_loss_train, best_loss_val, best_scheduler, best_scheduler_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lr_on_plateau(model, train_loader, val_loader, optimizer, device, num_epochs):\n",
    "    grid_params_ReduceLROnPlateau = {\n",
    "        'mode': ['max'],\n",
    "        'factor': [0.1],\n",
    "        'patience': [2],\n",
    "    }\n",
    "\n",
    "    best_f1 = float('-inf')\n",
    "    best_f1_train = None\n",
    "    best_f1_val = None\n",
    "    best_acc_train = None\n",
    "    best_acc_val = None\n",
    "    best_loss_train = None\n",
    "    best_loss_val = None\n",
    "    best_scheduler = None\n",
    "    best_scheduler_params = None\n",
    "\n",
    "    for mode in grid_params_ReduceLROnPlateau['mode']:\n",
    "        for factor in grid_params_ReduceLROnPlateau['factor']:\n",
    "            for patience in grid_params_ReduceLROnPlateau['patience']:\n",
    "                scheduler = ReduceLROnPlateau(optimizer, mode=mode, factor=factor, patience=patience)\n",
    "                \n",
    "                print(f\"Using ReduceLROnPlateau with mode={mode}, factor={factor}, patience={patience}\")\n",
    "                f1_train, f1_val, acc_train, acc_val, loss_train, loss_val = train_with_validation(\n",
    "                    model, train_loader, val_loader, optimizer, device, num_epochs, scheduler\n",
    "                )\n",
    "\n",
    "                if f1_val[-1] > best_f1:\n",
    "                    best_f1 = f1_val[-1]\n",
    "                    best_f1_train = f1_train\n",
    "                    best_f1_val = f1_val\n",
    "                    best_acc_train = acc_train\n",
    "                    best_acc_val = acc_val\n",
    "                    best_loss_train = loss_train\n",
    "                    best_loss_val = loss_val\n",
    "                    best_scheduler = scheduler\n",
    "                    best_scheduler_params = {'mode': mode, 'factor': factor, 'patience': patience}\n",
    "\n",
    "    return best_f1_train, best_f1_val, best_acc_train, best_acc_val, best_loss_train, best_loss_val, best_scheduler, best_scheduler_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_annealing_lr(model, train_loader, val_loader, optimizer, device, num_epochs):\n",
    "    grid_params_CosineAnnealingLR = {\n",
    "        'T_max': [50],\n",
    "        'eta_min': [1e-5]\n",
    "    }\n",
    "\n",
    "    best_f1 = float('-inf')\n",
    "    best_f1_train = None\n",
    "    best_f1_val = None\n",
    "    best_acc_train = None\n",
    "    best_acc_val = None\n",
    "    best_loss_train = None\n",
    "    best_loss_val = None\n",
    "    best_scheduler = None\n",
    "    best_scheduler_params = None\n",
    "\n",
    "    for T_max in grid_params_CosineAnnealingLR['T_max']:\n",
    "        for eta_min in grid_params_CosineAnnealingLR['eta_min']:\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
    "            \n",
    "            print(f\"Using CosineAnnealingLR with T_max={T_max} and eta_min={eta_min}\")\n",
    "            f1_train, f1_val, acc_train, acc_val, loss_train, loss_val = train_with_validation(\n",
    "                model, train_loader, val_loader, optimizer, device, num_epochs, scheduler\n",
    "            )\n",
    "\n",
    "            if f1_val[-1] > best_f1:\n",
    "                best_f1 = f1_val[-1]\n",
    "                best_f1_train = f1_train\n",
    "                best_f1_val = f1_val\n",
    "                best_acc_train = acc_train\n",
    "                best_acc_val = acc_val\n",
    "                best_loss_train = loss_train\n",
    "                best_loss_val = loss_val\n",
    "                best_scheduler = scheduler\n",
    "                best_scheduler_params = {'T_max': T_max, 'eta_min': eta_min}\n",
    "\n",
    "    return best_f1_train, best_f1_val, best_acc_train, best_acc_val, best_loss_train, best_loss_val, best_scheduler, best_scheduler_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_lr_grid(model, train_loader, val_loader, optimizer, device, num_epochs):\n",
    "\n",
    "    best_f1 = float('-inf')\n",
    "    best_f1_train = None\n",
    "    best_f1_val = None\n",
    "    best_acc_train = None\n",
    "    best_acc_val = None\n",
    "    best_loss_train = None\n",
    "    best_loss_val = None\n",
    "    best_scheduler = None\n",
    "    best_scheduler_params = None\n",
    "\n",
    "\n",
    "    f1_train, f1_val, acc_train, acc_val, loss_train, loss_val, scheduler, scheduler_params = exponential_lr(\n",
    "        model, train_loader, val_loader, optimizer, device, num_epochs\n",
    "    )\n",
    "    if f1_val[-1] > best_f1:\n",
    "        best_f1 = f1_val[-1]\n",
    "        best_f1_train = f1_train\n",
    "        best_f1_val = f1_val\n",
    "        best_acc_train = acc_train\n",
    "        best_acc_val = acc_val\n",
    "        best_loss_train = loss_train\n",
    "        best_loss_val = loss_val\n",
    "        best_scheduler = scheduler\n",
    "        best_scheduler_params = scheduler_params\n",
    "\n",
    "    f1_train, f1_val, acc_train, acc_val, loss_train, loss_val, scheduler, scheduler_params = step_lr(\n",
    "        model, train_loader, val_loader, optimizer, device, num_epochs\n",
    "    )\n",
    "    if f1_val[-1] > best_f1:\n",
    "        best_f1 = f1_val[-1]\n",
    "        best_f1_train = f1_train\n",
    "        best_f1_val = f1_val\n",
    "        best_acc_train = acc_train\n",
    "        best_acc_val = acc_val\n",
    "        best_loss_train = loss_train\n",
    "        best_loss_val = loss_val\n",
    "        best_scheduler = scheduler\n",
    "        best_scheduler_params = scheduler_params\n",
    "\n",
    "    f1_train, f1_val, acc_train, acc_val, loss_train, loss_val, scheduler, scheduler_params = multi_step_lr(\n",
    "        model, train_loader, val_loader, optimizer, device, num_epochs\n",
    "    )\n",
    "    if f1_val[-1] > best_f1:\n",
    "        best_f1 = f1_val[-1]\n",
    "        best_f1_train = f1_train\n",
    "        best_f1_val = f1_val\n",
    "        best_acc_train = acc_train\n",
    "        best_acc_val = acc_val\n",
    "        best_loss_train = loss_train\n",
    "        best_loss_val = loss_val\n",
    "        best_scheduler = scheduler\n",
    "        best_scheduler_params = scheduler_params\n",
    "\n",
    "    f1_train, f1_val, acc_train, acc_val, loss_train, loss_val, scheduler, scheduler_params = reduce_lr_on_plateau(\n",
    "        model, train_loader, val_loader, optimizer, device, num_epochs\n",
    "    )\n",
    "    if f1_val[-1] > best_f1:\n",
    "        best_f1 = f1_val[-1]\n",
    "        best_f1_train = f1_train\n",
    "        best_f1_val = f1_val\n",
    "        best_acc_train = acc_train\n",
    "        best_acc_val = acc_val\n",
    "        best_loss_train = loss_train\n",
    "        best_loss_val = loss_val\n",
    "        best_scheduler = scheduler\n",
    "        best_scheduler_params = scheduler_params\n",
    "\n",
    "    f1_train, f1_val, acc_train, acc_val, loss_train, loss_val, scheduler, scheduler_params = cosine_annealing_lr(\n",
    "        model, train_loader, val_loader, optimizer, device, num_epochs\n",
    "    )\n",
    "    if f1_val[-1] > best_f1:\n",
    "        best_f1 = f1_val[-1]\n",
    "        best_f1_train = f1_train\n",
    "        best_f1_val = f1_val\n",
    "        best_acc_train = acc_train\n",
    "        best_acc_val = acc_val\n",
    "        best_loss_train = loss_train\n",
    "        best_loss_val = loss_val\n",
    "        best_scheduler = scheduler\n",
    "        best_scheduler_params = scheduler_params\n",
    "\n",
    "    return best_f1_train, best_f1_val, best_acc_train, best_acc_val, best_loss_train, best_loss_val, best_scheduler, best_scheduler_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
