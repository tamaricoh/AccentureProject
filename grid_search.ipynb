{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from transformers import BertTokenizer, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from projAll import filtered_labels_at_least_5_list, CustomBertModel, create_dataset, train_with_validation, test, combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [5e-5, 3e-5, 1e-5],\n",
    "    'batch_size': [16, 32],\n",
    "    'num_epochs': [4, 6]\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "label_mapping = {label: idx for idx, label in enumerate(filtered_labels_at_least_5_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('csv/merged_aug.csv')\n",
    "df = combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, test_df = train_test_split(df,\n",
    "                                         test_size=0.2,\n",
    "                                         stratify=df['Artifact Id'],\n",
    "                                         random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df,\n",
    "                                         test_size=0.2,\n",
    "                                         stratify=train_val_df['Artifact Id'],\n",
    "                                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(acc_train, acc_val, acc_test, f1_train, f1_val, f1_test, params):\n",
    "    num_epochs = params['num_epochs']\n",
    "    learning_rate = params['learning_rate']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))  \n",
    "\n",
    "    # Plot Accuracy\n",
    "    axes[0].plot(epochs, acc_train, color='blue', linestyle='-', label='Train Accuracy')\n",
    "    axes[0].plot(epochs, acc_val, color='red', linestyle='-', label='Validation Accuracy')\n",
    "    axes[0].set_title('Accuracy Over Epochs')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Accuracy (%)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Plot F1 Score\n",
    "    axes[1].plot(epochs, f1_train, color='blue', linestyle='-', label='Train F1 Score')\n",
    "    axes[1].plot(epochs, f1_val, color='red', linestyle='-', label='Validation F1 Score')\n",
    "    axes[1].set_title('F1 Score Over Epochs')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('F1 Score')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    test_results_text = f\"Test Accuracy: {acc_test:.2f}%\\nTest F1 Score: {f1_test:.4f}\"\n",
    "    props = dict(boxstyle='round,pad=0.5', facecolor='lightgray', edgecolor='black')\n",
    "    fig.text(0.5, -0.05, test_results_text, fontsize=10, bbox=props, ha='center')\n",
    "\n",
    "    fig.suptitle(f\"Training, Validation, and Test Metrics\\n\"\n",
    "                 f\"Epochs: {num_epochs}, Learning Rate: {learning_rate}, Batch Size: {batch_size}\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to make room for suptitle\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'learning_rate': 5e-05, 'batch_size': 16, 'num_epochs': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13566f678a840609c3e518e9a407af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1/4:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def grid_search(param_grid):\n",
    "    best_f1 = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    grid_combinations = list(product(*param_grid.values()))\n",
    "    \n",
    "    train_dataset = create_dataset(train_df, tokenizer, label_mapping)\n",
    "    val_dataset = create_dataset(val_df, tokenizer, label_mapping)\n",
    "    test_dataset = create_dataset(test_df, tokenizer, label_mapping)\n",
    "    \n",
    "    \n",
    "    for params in grid_combinations:\n",
    "        current_params = dict(zip(param_grid.keys(), params))\n",
    "        \n",
    "        print(f\"Training with params: {current_params}\")\n",
    "        \n",
    "        learning_rate = current_params['learning_rate']\n",
    "        batch_size = current_params['batch_size']\n",
    "        num_epochs = current_params['num_epochs']\n",
    "        \n",
    "        model = CustomBertModel(num_labels=len(filtered_labels_at_least_5_list))\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "        model.to(device)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "        val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
    "        test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
    "\n",
    "        f1_train, f1_val, acc_train, acc_val = train_with_validation(model, train_loader, val_loader, optimizer, device, num_epochs)\n",
    "\n",
    "        predictions, true_labels = test(model, test_loader, device)\n",
    "        \n",
    "        f1_test = f1_score(true_labels, predictions, average='weighted')\n",
    "        acc_test = accuracy_score(true_labels, predictions) * 100\n",
    "\n",
    "        plot(acc_train, acc_val, acc_test, f1_train, f1_val, f1_test, current_params)\n",
    "        \n",
    "        if f1_test > best_f1:\n",
    "            best_f1 = f1_test\n",
    "            best_params = current_params\n",
    "    \n",
    "    print(f\"Best F1 Score: {best_f1:.4f} with parameters: {best_params}\")\n",
    "    return best_params\n",
    "    # return acc_train, acc_val, acc_test, f1_train, f1_val, f1_test, best_params\n",
    "\n",
    "params = grid_search(param_grid)\n",
    "# acc_train, acc_val, acc_test, f1_train, f1_val, f1_test, params = grid_search(param_grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
